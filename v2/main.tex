\documentclass[a4paper,11pt]{article}

\usepackage{mathpartir}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{color}

\input{ldefs}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}


\title{Adaptivity analysis}

\author{}

\date{January 19, 2019}

\begin{document}

\maketitle

\paragraph{Adaptivity}
Adaptivity is a measure of the nesting depth of a mechanism. To
represent this depth, we use extended natural numbers. Define $\natb =
\nat \cup \{\bot\}$, where $\bot$ is a special symbol and $\natbi =
\natb \cup \{\infty\}$. We use $\nnatA, \nnatB$ to range over $\nat$,
$\nnatbA, \nnatbB$ to range over $\natb$, and $\nnatbiA, \nnatbiB$ to
range over $\natbi$.

The functions $\max$ and $+$, and the order $\leq$ on natural numbers
extend to $\natbi$ in the natural way:
\[\begin{array}{lcl}
\max(\bot, \nnatbiA) & = & \nnatbiA \\
\max(\nnatbiA, \bot) & = & \nnatbiA \\
\max(\infty, \nnatbiA) & = & \infty \\
\max(\nnatbiA, \infty) & = & \infty \\
\\
%
\bot + \nnatbiA & = & \bot \\
\nnatbiA + \bot & = & \bot \\
\infty + \nnatbiA & = & \infty ~~~~ \mbox{if } \nnatbiA \neq \bot \\
\nnatbiA + \infty & = & \infty ~~~~ \mbox{if } \nnatbiA \neq \bot \\
\\
%
\bot \leq \nnatbiA \\
\nnatbiA \leq \infty
\end{array}
\]
One can think of $\bot$ as $-\infty$, with the special proviso that,
here, $-\infty + \infty$ is specifically defined to be $-\infty$.

\paragraph{Language}
Expressions are shown below. $\econst$ denotes constants (of some base
type $\tbase$, which may, for example, be reals or rational
numbers). $\eop$ represents a primitive operation (such as a
mechanism), which determines adaptivity. For simplicity, we assume
that $\eop$ can only have type $\tbase \to \tbool$. We make
environments explicit in closures. This is needed for the tracing
semantics later.
\[\begin{array}{llll}
\mbox{Expr.} & \expr & ::= & x ~|~ \expr_1 \eapp \expr_2 ~|~ \efix f(x).\expr
 ~|~ (\expr_1, \expr_2) ~|~ \eprojl(\expr) ~|~ \eprojr(\expr) ~| \\
%
& & & \etrue ~|~ \efalse ~|~ \eif(\expr_1, \expr_2, \expr_3) ~|~
\econst ~|~ \eop(\expr) \\
%
\mbox{Value} & \valr & ::= & \etrue ~|~ \efalse ~|~ \econst ~|~
(\efix f(x).\expr, \env) ~|~ (\valr_1, \valr_2) \\
%
\mbox{Environment} & \env & ::= & x_1 \mapsto \valr_1, \ldots, x_n \mapsto \valr_n
\end{array}\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Tracing operational semantics and adaptivity}

\paragraph{Traces}
A trace $\tr$ is a representation of the big-step derivation of an
expression's evaluation. Our big-step semantics output a trace. We use
traces to define the adaptivity of a run. Our notion of traces and the
tracing semantics is taken from~\cite[Section 4]{perera:dep}, but we
omit their ``holes'' for which we have no need. The construct
$\trapp{\tr_1}{\tr_2}{f}{x}{\tr_3}$ records a trace of function
application. $\tr_1$ is the trace of the head, $\tr_2$ the trace of
the argument and $\tr_3$ is the trace of the function body. $f$ and
$x$ are bound in $\tr_3$.
%
\[\begin{array}{llll}
\mbox{Trace} & \tr & ::= & x ~|~ \trapp{\tr_1}{\tr_2}{f}{x}{\tr_3} ~|~
\trfix f(x).e ~|~ (\tr_1, \tr_2) ~|~ \trprojl(\tr) ~|\\ 
%
& & & \trprojr(\tr) ~|~ \trtrue ~|~ \trfalse ~|~ \trift(\tr_b, \tr_t)
~|~ \triff(\tr_b, \tr_f) ~|~ \trconst ~|~ \trop(\tr)
\end{array}\]


\paragraph{Big-step tracing semantics}
The big-step, tracing semantics $\env, \expr \bigstep \valr, \tr$
computes a value $\valr$ and a trace $\tr$ from an expression $\expr$
and an enviroment $\env$ which maps the free variables of $\expr$ to
\emph{closed} values. The rules, taken from~\cite{perera:dep}, are
shown in Figure~\ref{fig:big-step}. Some salient points:
\begin{itemize}
\item[-] Erasing the traces from the semantics yields a standard
  big-step semantics.
\item[-] The trace of a primitive application $\eop(\expr)$
  records that $\eop$ was applied to the trace of
  $\expr$. This enables us to define adaptivity from a trace later.
\item[-] The trace of a variable $x$ is $x$. This way traces record
  where substitutions occur and, hence, variable dependencies. This is
  also needed for defining adaptivity.
\end{itemize}

\begin{figure}
\begin{mathpar}
  \inferrule{ }{\env, x \bigstep \env(x), x}
  %
  \and
  %
  \inferrule{ }{\env, \econst \bigstep \econst, \trconst}
  %
  \and
  %
  \inferrule{ }{\env, \etrue \bigstep \etrue, \trtrue}
  %
  \and
  %
  \inferrule{ }{\env, \efalse \bigstep \efalse, \trfalse}
  %
  \and
  %
  \inferrule{
  }{
    \env, \efix f(x). \expr \bigstep (\efix f(x).\expr, \env), \trfix f(x).\expr
  }
  %
  \and
  %
  \inferrule{
    \env, \expr_1 \bigstep \valr_1, \tr_1 \\
    \valr_1 = (\efix f(x).\expr, \env') \\\\
    \env, \expr_2 \bigstep \valr_2, \tr_2 \\
    \env'[f \mapsto \valr_1, x \mapsto \valr_2], \expr \bigstep \valr, \tr
  }{
    \env, \expr_1 \eapp \expr_2 \bigstep \valr, \trapp{\tr_1}{\tr_2}{f}{x}{\tr}
  }
  %
  \and
  %
  \inferrule{
    \env, \expr_1 \bigstep \valr_1, \tr_1 \\
    \env, \expr_2 \bigstep \valr_2, \tr_2
  }{
    \env, (\expr_1, \expr_2) \bigstep (\valr_1, \valr_2), (\tr_1, \tr_2)
  }
  %
  \and
  %
  \inferrule{
    \env, \expr \bigstep (\valr_1, \valr_2), \tr
  }{
    \env, \eprojl(\expr) \bigstep \valr_1, \trprojl(\tr)
  }
  %
  \and
  %
  \inferrule{
    \env, \expr \bigstep (\valr_1, \valr_2), \tr
  }{
    \env, \eprojr(\expr) \bigstep \valr_2, \trprojr(\tr)
  }
  %
  \and
  %
  \inferrule{
    \env, \expr \bigstep \etrue, \tr \\
    \env, \expr_1 \bigstep \valr, \tr_1
  }{
    \env, \eif(\expr, \expr_1, \expr_2) \bigstep \valr, \trift(\tr, \tr_1)
  }
  %
  \and
  %
  \inferrule{
    \env, \expr \bigstep \efalse, \tr \\
    \env, \expr_2 \bigstep \valr, \tr_2
  }{
    \env, \eif(\expr, \expr_1, \expr_2) \bigstep \valr, \triff(\tr, \tr_2)
  }
  %
  \and
  %
  \inferrule{
    \env, \expr \bigstep \valr, \tr \\
    \eop{}(\valr) = \valr'
  }{
    \env, \eop(\expr) \bigstep \valr', \trop(\tr)
  }
\end{mathpar}
  
  \caption{Big-step semantics with provenance}
  \label{fig:big-step}
\end{figure}


\paragraph{Adaptivity of a trace}
We define the \emph{adaptivity} of a trace $\tr$, $\adap(\tr)$, which
means the maximum number of nested $\eop$s in $\tr$, taking variable
and control dependencies into account. To define this, we need an
auxiliary notion called the \emph{depth of variable $x$} in trace
$\tr$, written $\ddep{x}(\tr)$, which is the maximum number of
$\eop{}$s in any path leading from the root of $\tr$ to an occurence
of $x$ (at a leaf), again taking variable and control dependencies
into account. Technically, $\adap: \mbox{Traces} \to \nat$ and
$\ddep{x}: \mbox{Traces} \to \natb$. If $x$ does not appear free in
$\tr$, $\ddep{x}(\tr)$ is $\bot$.

The functions $\adap$ and $\ddep{x}$ are defined by mutual induction
in Figure~\ref{fig:adap}. 

\begin{figure}
  \framebox{$\adap: \mbox{Traces} \to \nat$}
  \begin{mathpar}
    \begin{array}{lcl}
      \adap(x) & = & 0 \\
      %
      \adap(\trapp{\tr_1}{\tr_2}{f}{x}{\tr_3}) & = &
      \adap(\tr_1) + \max(\adap(\tr_3), \adap(\tr_2) + \ddep{x}(\tr_3))\\
      %
      \adap(\trfix f(x).\expr) & = & 0 \\
      %
      \adap((\tr_1, \tr_2)) & = & \max(\adap(\tr_1), \adap(\tr_2)) \\
      %
      \adap(\trprojl(\tr)) & = & \adap(\tr) \\
      %
      \adap(\trprojr(\tr)) & = & \adap(\tr) \\
      %
      \adap(\trtrue) & = & 0 \\
      %
      \adap(\trfalse) & = & 0 \\
      %
      \adap(\trift(\tr_b, \tr_t)) & = & \adap(\tr_b) + \adap(\tr_t) \\
      %
      \adap(\triff(\tr_b, \tr_f)) & = & \adap(\tr_b) + \adap(\tr_f) \\
      %
      \adap(\trconst) & = & 0 \\
      %
      \adap(\trop(\tr)) & = & 1 + \adap(\tr)
      \end{array}
  \end{mathpar}
  
  \framebox{$\ddep{x}: \mbox{Traces} \to \natb$}
  \begin{mathpar}
    \begin{array}{lcl}
      \ddep{x}(y) & = &
      \left\lbrace
      \begin{array}{ll}
        0 & \mbox{if } x = y \\
        \bot & \mbox{if } x \neq y
      \end{array}
      \right.\\
      %
      \ddep{x}(\trapp{\tr_1}{\tr_2}{f}{y}{\tr_3}) & = & \max(\ddep{x}(\tr_1), \\
      & & ~~~~~~~\adap(\tr_1) + \max(\ddep{x}(\tr_3), \ddep{x}(\tr_2) + \ddep{y}(\tr_3))) \\
      %
      \ddep{x}(\trfix f(y).\expr) & = & \bot \\
      %
      \ddep{x}((\tr_1, \tr_2)) & = & \max(\ddep{x}(\tr_1), \ddep{x}(\tr_2)) \\
      %
      \ddep{x}(\trprojl(\tr)) & = & \ddep{x}(\tr) \\
      %
      \ddep{x}(\trprojr(\tr)) & = & \ddep{x}(\tr) \\
      %
      \ddep{x}(\trtrue) & = & \bot \\
      %
      \ddep{x}(\trfalse) & = & \bot \\
      %
      \ddep{x}(\trift(\tr_b, \tr_t)) & = & \max(\ddep{x}(\tr_b), \adap(\tr_b) + \ddep{x}(\tr_t)) \\
      %
      \ddep{x}(\trift(\tr_b, \tr_f)) & = & \max(\ddep{x}(\tr_b), \adap(\tr_b) + \ddep{x}(\tr_f)) \\
      %
      \ddep{x}(\trconst) & = & \bot \\
      %
      \ddep{x}(\trop(\tr)) & = & 1 + \ddep{x}(\tr)
    \end{array}
  \end{mathpar}
  \caption{Adaptivity of a trace and depth of variable $x$ in a trace}
  \label{fig:adap}
\end{figure}

\paragraph{Explanation of $\adap$}
We explain the interesting cases of the definition of $\adap$. The
case $\trapp{\tr_1}{\tr_2}{f}{x}{\tr_3}$ corresponds to a function
application with $\tr_1$, $\tr_2$, $\tr_3$ being the traces of the
head, the argument and the body, respectively, and $x$ being the
argument. The adaptivity is defined to be $\adap(\tr_1) +
\max(\adap(\tr_3), \adap(\tr_2) + \ddep{x}(\tr_3))$. The term
$\adap(\tr_1)$ occurs additively since the entire computation is
control-dependent on the function the head of the application
evaluates to. The rest of the term $\max(\adap(\tr_3), \adap(\tr_2) +
\ddep{x}(\tr_3))$ is simply the maximum nesting depth in the body,
taking the data dependency on the argument into account. To see this,
consider the following exhaustive cases:
\begin{itemize}
  \item[-] When $x$ appears free in the trace $\tr_3$,
    $\ddep{x}(\tr_3)$ is the maximum $\eop$-nesting depth of $x$ in
    the body. Hence, $\max(\adap(\tr_3), \adap(\tr_2) +
    \ddep{x}(\tr_3))$ represents the maximum number of nested $\eop$s
    in the evaluation of $e[e'/x]$ where $e'$ is the argument
    expression that generates the trace $\tr_2$ and $e$ is the body of
    the function.
  \item[-] When $x$ does not appear free in the trace $\tr_3$ of the
    body (i.e., the body's evaluation does not depend on $x$),
    $\ddep{x}(\tr_3) = \bot$, so $\max(\adap(\tr_3), \adap(\tr_2) +
    \ddep{x}(\tr_3)) = \max(\adap(\tr_3), \adap(\tr_2) + \bot) =
    \max(\adap(\tr_3), \bot) = \adap(\tr_3)$, which is the adaptivity
    of the body in the absence of dependency from $x$.
\end{itemize}

The case $\trift(\tr_b, \tr_t)$ corresponds to the evaluation of
$\eif(\expr_b, \expr_t, \_)$ where $\expr_b$ evaluates to $\etrue$
with trace $\tr_b$ and $\tr_t$ is the trace of $\expr_t$. In this
case, since the entire evaluation of $\expr_t$ is control dependent on
$\expr_b$, the adaptivity is simply $\adap(\tr_b) + \adap(\tr_t)$.

\paragraph{Explanation of $\ddep{x}$}
We explain interesting cases in the definition of $\ddep{x}$.  For the
trace $\trapp{\tr_1}{\tr_2}{f}{x}{\tr_3}$, $\ddep{x}$ is defined as
$\max(\ddep{x}(\tr_1), \adap(\tr_1) + \max(\ddep{x}(\tr_3),
\ddep{x}(\tr_2) + \ddep{y}(\tr_3)))$. Here, $\max(\ddep{x}(\tr_3),
\ddep{x}(\tr_2) + \ddep{y}(\tr_3))$ is the maximum depth of $x$ in the
body ($\tr_3$), taking the dependency on the argument into
account. Specifically, when the argument variable $y$ is not used in
the body, $\ddep{y}(\tr_3) = \bot$, and this term is
$\ddep{x}(\tr_3)$.  The term $\adap(\tr_1)$ is added since the body's
entire execution is control-flow dependent on the function that the
head of the application evaluates to.

For the trace $\trift(\tr_b, \tr_t)$, $\ddep{x}$ is defined as
$\max(\ddep{x}(\tr_b), \adap(\tr_b) + \ddep{x}(\tr_t))$. The term
$\ddep{x}(\tr_b)$ is simply the maximum depth of $x$ in $\tr_b$. We
take the $\max$ of this with $\adap(\tr_b) + \ddep{x}(\tr_t)$, the
maximum depth of $x$ in $\tr_t$, taking the control dependency on
$\tr_b$ into account. Note that when $x$ is not used in $\tr_t$, then
$\ddep{x}(\tr_t) = \bot$ and $\ddep{x}(\trift(\tr_b, \tr_t)) =
\ddep{x}(\tr_b)$.

\begin{lem}\label{lem:ddep-leq-adap}
For all $\tr$ and $x$, $\ddep{x}(\tr) \leq \adap(\tr)$ in $\natb$.
\end{lem}
%
\begin{proof}
By easy induction on $\tr$, following the definitions of $\ddep{x}$
and $\adap$.
\end{proof}

\paragraph{Remark}
At first glance it may seem that Lemma~\ref{lem:ddep-leq-adap} can be
used to simplify the definition of $\ddep{x}(\trift(\tr_b, \tr_t))$
from $\max(\ddep{x}(\tr_b), \adap(\tr_b) + \ddep{x}(\tr_t))$ to
$\adap(\tr_b) + \ddep{x}(\tr_t)$ since $\ddep{x}(\tr_b) \leq
\adap(\tr_b)$. However, this simplification is not correct, since
$\ddep{x}(\tr_t)$ may be $\bot$. In that case, $\max(\ddep{x}(\tr_b),
\adap(\tr_b) + \ddep{x}(\tr_t))$ equals $\ddep{x}(\tr_b)$ while
$\adap(\tr_b) + \ddep{x}(\tr_t)$ equals $\bot$.

More generally, since $\bot$ behaves like $-\infty$, we do not have
the implication $a \leq b \Rightarrow a \leq b + c$ as $c$ may be
$-\infty$ ($\bot$). As a result, $a \leq b$ does not imply $\max(a, b
+ c) = b + c$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Type system}

We build a type system that statically approximates both $\adap$ and
$\ddep{x}$ for every $x$ in the context.

A \emph{depth map}, denoted $\dmap$, is a map from variables to
$\natbi$.
%
%% We define the support of a depth map, $\supp(\dmap) \defeq \{x ~|~
%% \dmap(x) \neq \bot\}$. We are only interested in depth maps with
%% finite support. Such a depth map can be written $x_1: \nnatbiA_1,
%% \ldots, x_k: \nnatbiA_k$.
%
Our typing judgment takes the form $\Gamma; \dmap \tvdash{\nnatA}
\expr: \type$, where $\Gamma$ is a typing context, $\dmap$ is a depth
map, and $\nnatA \in \nat$. The idea is that $\dmap$ gives an upper
bound on the depth of each free variable of $\expr$. Obviously, we
only care about the values of $\dmap$ on the domain of $\Gamma$ (at
the remaining points, $\dmap$ can be $\bot$; such a $\dmap$ can always
be finitely represented as $x_1: \nnatbiA_1, \ldots, x_k: \nnatbiA_k$,
where $x_1,\ldots,x_k$ are bound by $\Gamma$). $\nnatA$ is an upper
bound on the adaptivity of $\expr$.

\paragraph{Types}
Types $\type$ are simple, except that the function type is annotated
with the adaptivity $\nnatA$ of the function body, a depth-map $\dmap$
which gives the depths of all free variables of the body (i.e.,
variables bound in outer scopes), and a depth $\nnatbiA \in \natbi$ of
the argument variable in the body. The function name, although free in
the body, always has depth $\infty$, so we don't write its depth
explicitly.\footnote{The function name is never substituted by a value
  with a nontrivial trace, so we can choose any depth for it. $\infty$
  is the most permissive depth, as will become clear from the typing
  rules shortly.}

\[
\begin{array}{llll}
  \mbox{Type} & \type & ::= & \tbase ~|~ \tbool ~|~ \type_1 \times
  \type_2 ~|~ \tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}
\end{array}
\]

%% Note that the type $\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}$
%% is well-formed only in a context $\Gamma$ that binds all variables in
%% the domain of $\dmap$. 

\paragraph{Typing rules}
The typing rules are shown in Figure~\ref{fig:type-rules}. In these
rules, we write $\nnatbiA + \dmap$ or $\dmap + \nnatbiA$ for the depth
map $\lambda x.\, (\nnatbiA + \dmap(x))$ defined on the same domain as
$\dmap$. We also lift $\max$ and $+$ pointwise to depth maps.
%% and define the constant map $\dmapb$ that maps everything to
%% $\bot$.
These are also fomally defined in Figure~\ref{fig:type-rules}.

\begin{figure}
  \begin{mathpar}
    \inferrule{
      \Gamma(x) = \type \\ 0 \leq \dmap(x) \mbox{ or equiv.\ } \dmap(x) \neq \bot
    }{
      \Gamma; \dmap \tvdash{\nnatA} x: \type
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap_1 \tvdash{\nnatA_1} \expr_1: (\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}) \\
      \Gamma; \dmap_2 \tvdash{\nnatA_2} \expr_2: \type_1 \\\\
      \nnatA' = \nnatA_1 + \max(\nnatA, \nnatA_2 + \nnatbiA) \\
      \dmap' = \max(\dmap_1, \nnatA_1 + \max(\dmap, \dmap_2 + \nnatbiA))
    }{
      \Gamma; \dmap' \tvdash{\nnatA'} \expr_1 \eapp \expr_2 : \type_2
    }
    %
    \and
    %
    \inferrule{
      \Gamma, f: (\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}), x: \type_1;
      \dmap[f: \infty, x: \nnatbiA]
      \tvdash{\nnatA}
      \expr: \type_2
    }{
      \Gamma; \dmap' \tvdash{\nnatA'} \efix f(x).\expr: (\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA})
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap_1 \tvdash{\nnatA_1} \expr_1: \type_1 \\
      \Gamma; \dmap_2 \tvdash{\nnatA_2} \expr_2: \type_2 \\\\
      \dmap' = \max(\dmap_1,\dmap_2) \\
      \nnatA' = \max(\nnatA_1,\nnatA_2)
    }{
      \Gamma; \dmap' \tvdash{\nnatA'} (\expr_1, \expr_2): \type_1 \times \type_2
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap \tvdash{\nnatA} \expr: \type_1 \times \type_2
    }{
      \Gamma; \dmap \tvdash{\nnatA} \eprojl(\expr): \type_1
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap \tvdash{\nnatA} \expr: \type_1 \times \type_2
    }{
      \Gamma; \dmap \tvdash{\nnatA} \eprojr(\expr): \type_2
    }
    %
    \and
    %
    \inferrule{
    }{
      \Gamma; \dmap \tvdash{\nnatA} \etrue: \tbool
    }
    %
    \and
    %
    \inferrule{
    }{
      \Gamma; \dmap \tvdash{\nnatA} \efalse: \tbool
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap_1 \tvdash{\nnatA_1} \expr_1: \tbool \\
      \Gamma; \dmap \tvdash{\nnatA} \expr_2: \type \\
      \Gamma; \dmap \tvdash{\nnatA} \expr_3: \type \\\\
      \nnatA' = \nnatA_1 + \nnatA \\
      \dmap' = \max(\dmap_1, \nnatA_1 + \dmap)
    }{
      \Gamma; \dmap' \tvdash{\nnatA'} \eif(\expr_1, \expr_2, \expr_3):  \type
    }
    %
    \and
    %
    \inferrule{
    }{
      \Gamma; \dmap \tvdash{\nnatA} \econst: \tbase
    }
    %
    \and
    %
    \inferrule{
      \Gamma; \dmap \tvdash{\nnatA} \expr: \tbase \\
      \nnatA' = 1 + \nnatA \\
      \dmap' = 1 + \dmap
    }{
      \Gamma; \dmap' \tvdash{\nnatA'} \eop(\expr): \tbool
    }
  \end{mathpar}

  \framebox{
  \begin{mathpar}
    \mbox{\textbf{where: }} \\
%    \dmapb \defeq \lambda x.\, \bot \\
    \nnatbiA + \dmap \defeq \dmap + \nnatbiA \defeq \lambda x.\, (\nnatbiA + \dmap(x)) \\
    \dmap_1 + \dmap_2 \defeq \lambda x.\, (\dmap_1(x) + \dmap_2(x)) \\
    \max(\dmap_1, \dmap_2) \defeq \lambda x.\, \max(\dmap_1(x), \dmap_2(x))
  \end{mathpar}}

  \caption{Typing rules}
  \label{fig:type-rules}
\end{figure}

The rules follow \emph{exactly} the definitions of $\adap()$ and
$\ddep{x}()$ to compute $\nnatA$ and $\dmap$ in the conclusion of the
typing rule for every construct. For instance, the typing rule for
$\expr_1 \eapp \expr_2$ does exactly what $\adap()$ and $\ddep{x}()$
do for the trace $\trapp{\tr_1}{\tr_2}{f}{x}{\tr_3}$.

\paragraph{Remark}
The astute reader might note that our type system looks very much like
a coeffect+effect system where the coeffect $\dmap$ estimates the
depth of each free variable and the effect $\nnatA$ estimates the
adaptivity. While this is true at a high-level, note there are two
essential differences between our type system and a coeffect+effect
system.
\begin{itemize}
\item[-] In some of our rule (those for $\expr_1 \eapp \expr_2$ and
  $\eif(\expr_1, \expr_2, \expr_3)$), the conclusion's coeffect
  depends on the effects of the premises. For instance, in the rule
  for $\expr_1 \eapp \expr_2$, the final effect $\dmap'$ depends on
  the coeffect $\nnatA_1$. This does not happen in standard
  coeffect+effect systems.
\item[-] Our type for functions internalizes the effect. In a standard
  coeffect + effect system, the effect is always on the typing
  judgment, and at the point of function construction ($\efix$), one
  anticipatively adds the effects of future function applications to
  the effect in the conclusion.
\end{itemize}
It may be possible to do away with both these differences by tracking
more information in the coeffects (e.g., what is the maximum
adaptivity of a function's argument?), but the details need to be
worked out and verified.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Logical relation and soundness}

Our type system is sound in the following sense.

\begin{thm}[Soundness]\label{thm:soundness}
If $\tvdash{\nnatA} \expr: \type$ and $\cdot, \expr \bigstep \valr,
\tr$ then $\adap(\tr) \leq \nnatA$.
\end{thm}

To prove this theorem, we build a logical relation on types. As usual,
the relation consists of a value relation and an expression
relation. We first show a non-step-indexed version of the relation in
Figure~\ref{fig:lr:non-step}. This relation is well-founded but
cannot, as usual, be used directly to prove the soundness of the
$\efix$ typing rule. We show this relation here purely for exposition
without the clutter of step-indices. We will step-index the relation
shortly.

\begin{figure}
  \begin{mathpar}
    \begin{array}{lll}
      \lrv{\tbool} & = & \{\etrue, \efalse\} \\
      %
      \lrv{\tbase} & = & \{\econst ~|~ \econst: \tbase \} \\
      %
      \lrv{\type_1 \times \type_2} & = & \{(\valr_1, \valr_2) ~|~ \valr_1 \in \lrv{\type_1} \conj \valr_2 \in \lrv{\type_2} \}\\
      %
      \lrv{\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}} & = &
      \{(\efix f(x).\expr, \env) ~|~ \forall \valr \in \lrv{\type_1}.\\
      & & 
      ~~~~~~(\theta[x \mapsto \valr, f \mapsto (\efix f(x).\expr, \env)], \expr) \in \lre{\dmap[x: \nnatbiA, f: \infty]}{\nnatA}{\type_2}\} \\
      %
      \\
      %
      \lre{\dmap}{\nnatA}{\type} & = & \{ (\env, \expr) ~|~ \forall \valr\, \tr.\, (\env, \expr \bigstep \valr, \tr) \\
      & & ~~~~~~~~~~~~~~~~~\Rightarrow (\adap(\tr) \leq \nnatA \conj \\
      & & ~~~~~~~~~~~~~~~~~~~~~~~\forall x \in \mbox{Vars}.\, \ddep{x}(\tr) \leq \dmap(x) \conj \\
      & & ~~~~~~~~~~~~~~~~~~~~~~~\valr \in \lrv{\type})
      \}
    \end{array}
  \end{mathpar}
  \caption{Logical relation without step-indexing}
  \label{fig:lr:non-step}
\end{figure}

The value relation $\lrv{\type}$, as usual, defines which
\emph{closed} values are semantically in the type $\type$. The only
interesting case of this relation is the case for the arrow type. The
expression relation $\lre{\dmap}{\nnatA}{\type}$ is more
interesting. It is indexed by a depth map and an
adaptivity. Importantly, this relation contains sets of configurations
$(\env, \expr)$ where the evaluation of $(\env, \expr)$ produces a
value in the value relation at $\type$, the adaptivity of the trace is
no more than $\nnatA$ and for any variable $x$, $\ddep{x}$ of the
trace is no more than $\dmap(x)$.

The value relation can be lifted to contexts in the usual way. Say
that $\env \in \lrv{\Gamma}$ when $\dom(\env) \supseteq \dom(\Gamma)$
and for every $x \in \dom(\Gamma)$, $\env(x) \in \lrv{\Gamma(x)}$.

The fundamental theorem we would \emph{like} to prove is the
following. Of course, this cannot be proven until we step-index the
relation, but this should give an idea of the connection between the
type system and the logical relation.

\begin{prop}[Fundamental theorem]
  If $\Gamma; \dmap \tvdash{\nnatA} \expr: \type$ and $\env \in
  \lrv{\Gamma}$, then $(\env, \expr) \in \lre{\dmap}{\nnatA}{\type}$.
\end{prop}

If this proposition were provable, then Theorem~\ref{thm:soundness}
would follow from it immediately.

\paragraph{Step-indexed logical relation}
To step-index the relation we need to have a notion of the number of
reduction steps. One way to do this is to change the operational
semantics to count the number of reduction steps. However, this is
unnecessary since the number of reduction steps in a derivation is
exactly the size of the derivation's trace. So, we just ``index'' the
relation on the size of the derivation's trace. The same technique has
been used previously~\cite{cicek15}. Formally, the size of a trace
$\tr$, written $\size{\tr}$, is the number of non-leaf constructors in
$\tr$.  

The step-indexed relation is shown in Figure~\ref{fig:lr:step}. Now,
the value relation $\lrv{\type}$ contains pairs of step-indices
$\stepiA$ and closed values. The expression relation contains pairs of
step-indices and configurations $(\env, \expr)$. The relation
basically mirrors the non-step-indexed relation of
Figure~\ref{fig:lr:non-step}, with step indexes added in the
completely standard way.


\begin{figure}
  \begin{mathpar}
    \begin{array}{lll}
      \lrv{\tbool} & = & \{(\stepiA, \etrue) ~|~ \stepiA \in \nat\} \cup
      \{ (\stepiA, \efalse) ~|~ \stepiA \in \nat\} \\
      %
      \lrv{\tbase} & = & \{(\stepiA, \econst) ~|~ \stepiA \in \nat \conj \econst: \tbase \} \\
      %
      \lrv{\type_1 \times \type_2} & = & \{(\stepiA, (\valr_1, \valr_2)) ~|~ (\stepiA, \valr_1) \in \lrv{\type_1} \conj (\stepiA, \valr_2) \in \lrv{\type_2} \}\\
      %
      \lrv{\tarr{\type_1}{\type_2}{\nnatbiA}{\dmap}{\nnatA}} & = &
      \{(\stepiA, (\efix f(x).\expr, \env)) ~|~ \forall \stepiB < \stepiA.\, \forall (\stepiB, \valr) \in \lrv{\type_1}.\\
      & & 
      ~~~~~~(\stepiB, (\theta[x \mapsto \valr, f \mapsto (\efix f(x).\expr, \env)], \expr)) \in \lre{\dmap[x: \nnatbiA, f: \infty]}{\nnatA}{\type_2}\} \\
      %
      \\
      %
      \lre{\dmap}{\nnatA}{\type} & = & \{ (\stepiA, (\env, \expr)) ~|~ \forall \valr\, \tr\, \stepiB.\, (\env, \expr \bigstep \valr, \tr) \conj (\size{\tr} = \stepiB) \conj (\stepiB \leq \stepiA) \\
      & & ~~~~~~~~~~~~~~~~~~~~~~~~~\Rightarrow (\adap(\tr) \leq \nnatA \conj \\
      & & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\forall x \in \mbox{Vars}.\, \ddep{x}(\tr) \leq \dmap(x) \conj \\
      & & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~((\stepiA - \stepiB,  \valr) \in \lrv{\type})
      \}
    \end{array}
  \end{mathpar}
  \caption{Logical relation with step-indexing}
  \label{fig:lr:step}
\end{figure}

We say that $(\stepiA, \env) \in \lrv{\Gamma}$ when $\dom(\env)
\supseteq \dom(\Gamma)$ and for every $x \in \dom(\Gamma)$, $(\stepiA,
\env(x)) \in \lrv{\Gamma(x)}$.

\begin{thm}[Fundamental theorem]
  If $\Gamma; \dmap \tvdash{\nnatA} \expr: \type$ and $(\stepiA, \env)
  \in \lrv{\Gamma}$, then $(\stepiA, (\env, \expr)) \in
  \lre{\dmap}{\nnatA}{\type}$.
\end{thm}
%
\begin{proof}
TODO. By induction on the given typing derivation. For the case of
$\efix$, we subinduct on the step index.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\bibliographystyle{plain}
\bibliography{adaptivity.bib}

\end{document}



