%
%
There are four steps to get the adaptivity of a program ${c}$ based on analyzing the program. 
\begin{enumerate}
\item Collecting the variables that are newly assigned in the program (via assignment expressions). These variables are stored in an labeled variable vector $\lvar$. 
We also track extra information of each assigned variable (whether it is assigned by a query result, or showing up in loop, or showing up in $\eif$ expression or o.w.) and store it in a vector $\flag$ of the same size as $\lvar$.
%
\item Tracking the data flow relations between all these labeled variables. These informations are stored in a matrix $\Mtrix$, whose size is $|\lvar_c| \times |\lvar_c|$. 
%
\item Estimating the reachability bound of each variable in $\lvar$.
%
\item With all these informations from previous steps, generating a program-based dependency graph $\progG$ and compute the adaptivity bound.
\end{enumerate}

In the following subsections, 
we first define the notations and symbols being used in \THESYSTEM  with a simple example for understanding these definitions. 
Then we present the algorithmic analysis rules, which is the core of the \THESYSTEM, with
3 examples illustrating how \THESYSTEM  works.
In the following subsections, we present the adaptivity analysis based on the \THESYSTEM's analyzing results, and the soundness w.r.t. the trace-based analyzing results in previous sections.

\subsection{Notations}
%
\label{subsec:alg_notation}
%
Consider the program $c$ below in the left hand side as an example, its labeled variables $\lvar$ (short for $\lvar({c})$) is as in the right hand side is shown as follows:
$$
{c} = 
\begin{array}{l}
\left[{\assign {x_1} {\query(0)}}		\right]^1;
\\
\left[{\assign {x_2} {x_1 + 1}}		\right]^2;
\\
\left[{\assign {x_3} {x_2 + 2}}		\right]^3
\end{array}
~~~~~~~~~~~~
\lvar_c = \left [ 
\begin{matrix}
{x_1} \\
{x_2} \\
{x_3} \\
\end{matrix} \right ]
$$
%
\begin{lem}
For any program ${c}$, every labeled variable in $\lvar_{c}$ is distinct
\end{lem}
%
{
\begin{defn}[Variable Flags ($\flag$)].
\\
Given a program  ${c}$ with its labeled variables $\lvar$, the $\flag$ is a vector of the same length as $\lvar$, s.t. for each variable ${x}$ showing up as the $i$-th element in $\lvar$ (i.e., ${x} = \lvar(i)$), 
$\flag(i) \in \{0, 1, 2\}$ is defined as follows:
%
%
\[
\flag(i) := 
\left\{
\begin{array}{ll}
2 & 
{x^l} \in \lvar_{c} \land 
(\exists {\qexpr}. ~ s.t., ~
[\assign{{x}}{\query({\qexpr})}]^l \in_{c} {c})
\\
1 &  
\begin{array}{l}
{x^l} \in \lvar_{c} \bigwedge \\
\left(
\begin{array}{l}
\big(\exists  ~ {c'}, {\expr}, \sbexpr, l, l'. ~
	\ewhile [\sbexpr]^l \edo {c'} \in_{c} {c}
	\land 
	[{\assign{x}{\expr}}]^{l'} \in_{c}  {c'}
\big) \bigvee
\\
\big(\exists ~ \sbexpr, l, l_1, l_2, {c_1}, {c_2}, {\expr}_1, {\expr}_2. ~
	\eif([\sbexpr]^l, {c_1}, {c_2}) \in_{c} {c} \land
	([{\assign{x}{\expr_1}}]^{l1} \in_{c} {c_1} \lor 
	[{\assign{x}{\expr_2}}]^{l2} \in_{c} {c_2})
\big)
\end{array}
\right)
\end{array}
\\
0 & \text{o.w.}
\end{array}
\right\}. 
\] 
%
\end{defn}
%
Operations on $\flag$ are defined as follows:
\begin{equation}
\begin{array}{llll}
{\flag_1 \uplus \flag_2}(i) & := &
\left\{
\begin{array}{ll}
k & k = \max{\big\{\flag_1(i), \flag_2(i)\big\}} 
\land |\flag_1| = |\flag_2|\\
0 & o.w.
\end{array}\right.
& i = 1, \cdots, |\flag_1|  
\\
{\flag \uplus n}(i) & := & 
\max\big\{ \flag(i), n \big\} 
& i = 1, \ldots, |\flag|    
\\
\left[ n \right]^k (i) & := &  n
& i = 1, \ldots, k ~ \land ~ |\left[ n \right]^k| = k
\end{array}
\end{equation}
%
Given a program  ${c}$ with its labeled variables $\lvar$,
and two variables ${x}$, ${y}$ showing up as $i$-th, $j$-th elements in $\lvar$ 
(i.e., ${x} = \lvar(i)$ and ${y} = \lvar(j)$),
we say ${y}$ flows to ${x}$ in ${c}$ if and only if $j < i$ and 
the value of ${y}$ directly or indirectly influence the evaluation of the value of ${x}$ as follows:
%
\begin{itemize}
\item (Explicit Influence) The program ${c}$ contains either 
a command $\assign{{x}}{\sexpr}$ or $\assign{{x}}{\query({\qexpr})}$,
such that ${y}$ shows up as a free variable in $\sexpr$ or ${\qexpr}$.
We use $\flowsto({x, y, c})$ to denote ${y}$ flows to ${x}$ in ${c}$.
%
\item (Implicit Influence) The program ${c}$ contains either a while loop
command
or if condition command, such that ${y}$ shows up in the guard
and ${x}$ shows up in the left hand of an assignment command in the body.
\end{itemize}
%
This is formally defined in \ref{def:flowsto}.
We use $FV(\expr)$, $FV(\sbexpr)$ and $FV(\qexpr)$ denote the set of free variables in 
expression $\expr$, boolean expression $\sbexpr$ and query expression $\qexpr$ respectively.
%
\\
\todo{Formal Definition Needed:
$\live^l(c) \subseteq \lvar_c$ 
is the set of labeled variables $x^l \in \lvar_c$ assumed to be live at the entry point of  executing the command of label $l$.}
%
\begin{defn}[Data Flows between Assigned Variables ($\flowsto$)].
\label{def:flowsto}
\\
In a program  ${c}$,
an variable ${x^i}  \in \lvar_c $ is in the \emph{flows to} relation with another variable ${y^j} \in \lvar_c$
in ${c}$, denoted as $\flowsto({x^i, y^j, c})$, is defined as follows:
%
\[
\begin{array}{l}
\flowsto({x^i, y^j, c}) \triangleq 
\\
\left( \bigvee
\begin{array}{l}
(\exists \sexpr . ~ [\assign{y}{\sexpr}]^j \in_{c} {c} 
\land {x} \in VAR(\sexpr) \land (x^i \in \live^j(c)))
\\
(\exists {\qexpr}. ~ [\assign{y}{\query({\qexpr})}]^j \in_{c} {c} 
\land x \in VAR({\qexpr}) \land (x^i \in \live^j(c))))
\\
\big(\exists  ~ {c_w}, {(\expr \lor \qexpr)}, \sbexpr, l \in \mathbb{N}. ~
	\ewhile [\sbexpr]^l \edo {c_w} \in_{c} {c}
	\land 
	[{\assign{y}{\expr \lor \query(\qexpr)}}]^{j} \in_{c}  {c_w}
\big) \land {x} \in VAR(\sbexpr) \land (x^i \in \live^l(c)))
\\
\big(
\exists ~ \sbexpr, l \in \mathbb{N}, {c_1}, {c_2}, {\expr}_1, {\expr}_2. ~
	\eif([\sbexpr]^l, {c_1}, {c_2}) \in_{c} {c} \land
	([{\assign{y}{\expr_1}}]^j \in_{c} {c_1} \lor 
	[{\assign{y}{\expr_2}}]^j \in_{c} {c_2})
\land {x} \in VAR(\sbexpr) \land (x^i \in \live^l(c)))
\big)
\\
(\exists z^r \in \lvar_c \st \flowsto(x^i, z^r, c) \land \flowsto(z^r, y^j, c))
\end{array}
\right).
\end{array}
\]
%
\end{defn}
}
%
%
\begin{defn}[Data Flow Matrix ($\Mtrix$)]
The data flow matrix $\Mtrix$ of a program $c$ is a matrix of size $|\lvar_c| \times |\lvar_c|$ 
s.t.,
%
\[
\Mtrix(i, j) \triangleq
\left\{
\begin{array}{ll}
1	&	\flowsto({x^i, y^j, c}) \\
0	& o.w.
\end{array}
\right., {x^i}, y^j  \in \lvar_c.
\]
%
\end{defn}
%
Operations on the data flow matrices are defined as follows:
%
\begin{equation}
\Mtrix_1 ; \Mtrix_2 
:= \Mtrix_2 \cdot \Mtrix_1 + \Mtrix_1 + \Mtrix_2
\end{equation}
%
Consider the same program $c$ as above, its data flow matrix $\Mtrix$ and $\flag$ for the program $c$ is as follows:
$$
{c} = 
\begin{array}{l}
\left[{\assign {x_1} {\query(0)}}	\right]^1;
\\
\left[{\assign {x_2} {x_1 + 1}}		\right]^2;
\\
\left[{\assign {x_3} {x_2 + 2}}		\right]^3
\end{array}
~~~~~~~~~~~~
\Mtrix
=  \left[ 
\begin{matrix}
0 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
\end{matrix} \right] ~ , 
\flag = \left [ \begin{matrix}
1 \\
0 \\
0 \\
\end{matrix} \right ]
$$
%
There are two special matrices used for generating the data flow matrix $\Mtrix$ in the analysis algorithm. They are the left matrix $\lMtrix_i$ and right matrix $\mathsf{R_{(e, i)}}$.

Given a program  ${c}$ with its labeled variables $\lvar$ of length $N$,
the left matrix $\lMtrix_i$ generates a matrix of $1$ column, $N$ rows, 
where the $i$-th row is $1$ and all the other rows are $0$.
%
\begin{defn}[Left Matrix ($\lMtrix_i$)].
\\
Given a program  ${c}$ with its labeled variables $\lvar$ of size $N$, 
the left matrix $\lMtrix_i$ is defined as follows:
\[
\lMtrix_i(j) : = 
\left
\{
\begin{array}{ll}
1 & j = i \\
0 & o.w.
\end{array}
\right.,
j = 1, \ldots, N.
\]
\end{defn}
%
Given a program  ${c}$ with its labeled variables $\lvar$ of length $N$,
the right matrix $\rMtrix_{\expr, i}$ generates a matrix of one row and $N$ columns, 
where the locations of free variables in $\expr$ is marked as $1$. 
%
%
\begin{defn}[Right Matrix ($\rMtrix_{\expr}$)].
\\
Given a program  ${c}$ with its labeled variables $\lvar$ of length $N$, 
the right matrix $\rMtrix_{\expr}$ is defined as follows:
\[
\rMtrix_{\expr}(j) : = 
\left\{
\begin{array}{ll}
1 & {x} \in FV(\expr) 
\\
0 & o.w.
\end{array}
\right.,
{x} = \lvar(j) ~ , ~ j = 1, \ldots, N.
\]
%
%
\end{defn}
%
Using the same example program ${c}$ as above with labeled variables $\lvar = [ {x_1 , x_2 , x_3} ] $,
the left and right matrices w.r.t. its $2$-nd command 
$\left[{\assign {x_2} {x_1 + 1}}\right]^2$  are as follows:
\[
\lMtrix_1 = \left[ \begin{matrix}
0   \\
1 	 \\
0   \\
\end{matrix}   \right ] 
~~~~~~~~~~~~~~
\rMtrix_{{x}_1 + 1}
= \left[ \begin{matrix} 
1 & 0 & 0 \\
\end{matrix}  \right]
\]
%
%
%
\subsection{Algorithmic Analysis Rules}
%
% \paragraph{Variable Collection Algorithm, $\varCol$}
% % The $\varCol$ algorithm shows how the labeled variables $\lvar$ are collected 
% % (via the command ${\assign{x}{\expr}}$ or ${\assign{x}{\query(\qexpr)}}$) from the program ${c}$ in the first step.
% % The algorithmic rules for $\varCol$ algorithm is defined in Figure~\ref{fig:var_col}. 
% % It has the form: $\ag{\lvar; w; {c}}{ \lvar'; w'} $. 
% % The input of $\varCol$ is the labeled variables $\lvar$ collected before the program ${c}$, a while map $w$ consistent with previous estimation, a program ${c}$. 
% % The output of the algorithm is the updated labeled variables $\lvar'$, along with the updated while map $w$ for next steps' collecting.   
% The $\varCol$ algorithm shows how the labeled variables $\lvar$ are collected 
% (via the command ${\assign{x}{\expr}}$ or ${\assign{x}{\query(\qexpr)}}$) from the program ${c}$ in the first step, 
% along with constructing the flag for each variable, i.e., $\flag$.
% The algorithmic rules for $\varCol$ algorithm is defined in Figure~\ref{fig:var_col}. 
% It has the form: 
% {$\ag{\lvar; \flag; {c}}{ \lvar'; \flag'} $}. 
% The input of $\varCol$ is a program ${c}$, 
% the labeled variables $\lvar$ collected before the program ${c}$ 
% as well as the flags $\flag$ for every corresponding variable .
% The output of the algorithm is the updated labeled variables $\lvar'$ and flags $\flag'$ thorough the program ${c}$
% %
% % We have the algorithmic rules for $\varCol$ algorithm of the form: $\ag{\lvar; w; {c}}{\lvar';w'} $ as in Figure \ref{fig:var_col}. 
% %
% \begin{figure}
% {
% \begin{mathpar}
% \inferrule
% {
% \empty
% }
% { \ag{\lvar ; \flag; {[\assign {x}{\expr}]^{l}}}
% {\lvar ++ [{x}]; \flag++[0]}
% }
% ~\textbf{\varCol-asgn}
% \and
% \inferrule
% {
% }
% { \ag{\lvar; \flag; [ \assign{{x}}{\query({\qexpr})}]^{l}}
% {\lvar ++ [{x}]; \flag ++ [2]} 
% }~\textbf{\varCol-query}
% %
% \and 
% %
% \inferrule
% {
% \ag{\lvar; [];  {c_1}}{\lvar_1; \flag_1}
% \and 
% \ag{\lvar_1; []; {c_2}}{ \lvar_2; \flag_2}
% \and
% \lvar_3 = \lvar_2 ++ \lvar'
% \and
% \flag_3 = \flag ++ ((\flag_1 ++ \flag_2) \uplus 1)
% }
% {
% \ag{\lvar; \flag;
% [\eif({\bexpr}, { c_1, c_2)}]^{l} }
% {\lvar_3; \flag_3}
% }~\textbf{\varCol-if}
% %
% %
% %
% \and 
% %
% \inferrule
% {
% \ag{\lvar; \flag {c_1}}{\lvar_1; \flag_1}
% \and 
% \ag{\lvar_1; \flag_1 ; {c_2}}{\lvar_2; \flag_2}
% }
% {
% \ag{\lvar; \flag;
% {(c_1 ; c_2)}}{\lvar_2 ; \flag_2}
% }
% ~\textbf{\varCol-seq}
% \and 
% %
% %
% {
% \inferrule
% {
% { \ag{\lvar; [] ; {c}}
% {\lvar'; \flag' }  }
% \\
% \lvar'' = \lvar'
% \and 
% \flag'' = \flag ++ (\flag' \uplus 1)
% }
% {
% \ag{\lvar; \flag;  
% \ewhile [{b}]^{l}
% \edo  {c} }{\lvar''; \flag''}
% }
% ~\textbf{\varCol-while}
% }
% \end{mathpar}
% }
% \caption{The Algorithmic Rules of $\varCol$ }
% \label{fig:var_col}
% \end{figure}
% %
% %
% The assignment commands are the source of variables $\varCol$ collecting, 
% in the case $\textbf{\varCol-asgn}$ and $\textbf{\varCol-query}$, 
% the output labeled variables are extended by ${x}$. 
% \\
% \todo{
% When it comes to the $\eif \ldots \ethen \ldots \eelse$ command in the rule $\textbf{\varCol-if}$, variables assigned in the then branch ${c_1}$, as well as the variables assigned in the else branch ${c_2}$, and the new generated variables $\bar{{x}},\bar{{y}},\bar{{z}}$ in $ [ \bar{{x}}, \bar{{x_1}}, \bar{{x_2}}] ,[ \bar{{y}}, \bar{{y_1}}, \bar{{y_2}}],[ \bar{{z}}, \bar{{z_1}}, \bar{{z_2}}]$.
% \\ 
% The sequence command ${c_1;c_2}$ is standard by accumulating the predicted variables in the two commands ${c_1}$ and ${c_2}$ preserving their order. 
% \\
% The while command $\ewhile {\bexpr}, [{\bar{x}}] \ldots \edo {c}$ considers the newly generated variables by SSA transformation ${\bar{x}}$
% as well and the newly labeled variables in its body ${c}$.
% \\
% %
% Below we present the definition for a valid index, to have a clear understanding on the variable collecting algorithm:
% }
% %
% %
% \todo{
% \begin{defn}[Valid Index (Remove?)]
% Given an assigned variable list $\lvar$, $\lvar; \vDash ({c},i_1,i_2)$ iff 
% $\lvar' = \lvar[0,\ldots, i_1-1], \lvar';{c} \to \lvar'' \land \lvar'' = \lvar[0, \ldots, i_2-1] $.  
% \end{defn}}
% %
% %
\todo{Data Dependency Analysis Algorithm Needed: (Possibly modify based on existing one, or a different one) get the more precise dependency information. 
i.e., instead of dependency on all the over-approximated variables, 
but dependency on only the variables assumed to be live.
}
\paragraph{Data Dependency Analysis Algorithm}
%
In this data flow matrix generating algorithm, we analyze the data flow information among all labeled variables $\lvar$ collected via the the $\varCol$ algorithm of length $N$.
%
We track the data flow relations between all these labeled variables. These informations are stored in a matrix $\Mtrix$, whose size is $N \times N$. 
% We also track whether arbitrary variable is assigned with a query result in a vector $\flag$ with size $|\lvar|$. 
%
The algorithm to fill in the matrix is of the form: 
{$\ad{\Gamma ; {c} ; \lvar}{\Mtrix}$}
$\ad{\Gamma ; {c} ; i_1, i_2}{\Mtrix; \flag}$. 
$\Gamma$ is a vector records the variables the current program ${c}$ depends on, the index $i_1$ is a pointer which refers to the position of the first new-generated variable in ${c}$ in the labeled variables $\lvar$, and $i_2$ points to the first new variable that is not in ${c}$ (if exists). 
% %
% %
% {
% \begin{defn}[Valid Gamma (Remove?)]
% $\Gamma \vDash i_1$ iff $\forall i \geq i_1, \Gamma(i_1)=0 $.  
% \end{defn}
% }
%%
%
% \framebox{$ {\Gamma} \vdash^{i_1, i_2}_{\Mtrix, \flag} ~ c $}
% \begin{mathpar}
% \inferrule
% {\Mtrix = \lMtrix_i * ( \rMtrix_{{\expr},i} + \Gamma )
% }
% {
%  \ad{\Gamma;[\assign {{x}}{{\expr}} ]^{l}; i }{\Mtrix; \flag_{0}; i+1 }
% }
% ~\textbf{\graphGen-asgn}
% \and
% {
% \inferrule
% {\Mtrix = \lMtrix_i * ( \rMtrix_{{\expr},i} + \Gamma )
% \\
% \flag = \lMtrix_i \and \flag(i) = 1
% }
% { 
% \ad{\Gamma;[ \assign{{x}}{\query({\expr})} ]^{l} ; i }
% {\Mtrix;\flag;i+1}
% }~\textbf{\graphGen-query}}
% %
% \and 
% %
% {
% \inferrule
% {
% {\ad{\Gamma + \rMtrix_{{\bexpr}, i_1}; {c_1} ; i_1 }{ \Mtrix_1;\flag_1;i_2 }}
% \and 
% {\ad{\Gamma + \rMtrix_{{\bexpr}, i_1};{c_2} ; i_2 }{ \Mtrix_2; \flag_2 ;i_3}}
% \\
% {\ad{\Gamma; [ \bar{{x}}, \bar{{x_1}}, \bar{{x_2}}]; i_3 }{ M_x; \flag_{\emptyset}; i_3+|\bar{{x}}| }}
% %
% \\
% %
% {\ad{\Gamma; [ \bar{{y}}, \bar{{y_1}}, \bar{{y_2}}]; i_3+|\bar{{x}}| }{ \Mtrix_y; \flag_{\emptyset}; i_3+|\bar{{x}}|+|\bar{{y}}| }}
% %
% \\
% %
% {\ad{\Gamma; [ \bar{{z}}, \bar{{z_1}}, \bar{{z_2}}]; i_3+|\bar{{x}}|+ |\bar{{y}}|}{ \Mtrix_y; \flag_{\emptyset}; i_3+|\bar{{x}}|+|\bar{{y}}| + |\bar{{z}}| }}
% \\
% {\Mtrix = (\Mtrix_1 + \Mtrix_2)+ \Mtrix_x+ \Mtrix_y + \Mtrix_z }
% }
% {
% \ad{\Gamma ; \eif([{\bexpr}]^{l},[ \bar{{x}}, \bar{{x_1}},
% \bar{{x_2}}] ,[ \bar{{y}}, \bar{{y_1}}, \bar{{y_2}}], 
% [ \bar{{z}}, \bar{{z_1}}, \bar{{z_2}}],
% { c_1, c_2)} ; i_1}{ \Mtrix ; \flag_1 \uplus \flag_2 \uplus 2  ; i_3+|\bar{x}|+|\bar{y}|+|\bar{z}| }
% }
% ~\textbf{\graphGen-if}
% }
% %
% %
% %
% \and 
% %
% \inferrule
% {
% {\ad{\Gamma; {c_1} ; i_1 }{ \Mtrix_1 ; \flag_1; i_2 }  }
% \and 
% {
% \ad{\Gamma;{c_2}; i_2}{ \Mtrix_2; \flag_2 ;i_3 }}
% }
% {
% \ad{\Gamma ; ({c_1 ; c_2} ) ; i_1}{( \Mtrix_1 {;} \Mtrix_2) ; \flag_1 \uplus V_2 ; i_3  }
% }
% ~\textbf{\graphGen-seq}
% %
% \and 
% %
% \and 
% %
% { 
% \inferrule
% {
% B= |{\bar{x}}| \and {A = |{c}|}
% \\
% {\ad{\Gamma;[\bar{{x}}, \bar{{x_1}}, \bar{{x_2}}]; i+ (B+A) }{ \Mtrix_{1};V_{1}; i+B+(B+A) }}
% \\
% {
% \ad{\Gamma;{c} ; i+B+(B+A)  }{ \Mtrix_{2}; \flag_{2}; i+B+A+(B+A) }
% }
% \\
% {
% \ad{\Gamma ; [\bar{{x}}, \bar{{x_1}}, \bar{{x_2}}] ; i+(B+A) }{ \Mtrix; \flag ;i+(B+A)+B}
% }
% \\
% { \Mtrix' = \Mtrix + ( \Mtrix_{1} + \Mtrix_{2}) }
% \and
% {
% \flag' = \flag \uplus (( \flag_{1} \uplus \flag_{2}) \uplus 2)  }
% }
% {
% \ad{\Gamma;
% \ewhile ~ [ b ]^{l} ~ {n} ~
% [\bar{{x}}, \bar{{x_1}}, \bar{{x_2}}] 
% ~ \edo ~  c;
% i }{ \Mtrix'; \flag' ;i+(B+A)+B }
% }~\textbf{\graphGen-while}
% }
% \end{mathpar}
{
\framebox{$ \ad{\Gamma; c; \lvar_c}{\Mtrix}$}
\begin{mathpar}
\inferrule
{
{x}^l \in \lvar_c
\and 
\Mtrix = \lMtrix_i * ( \rMtrix_{{\expr}} + \Gamma )
}
{
\ad{\Gamma; [\assign {{x}}{{\expr}} ]^{l}; \lvar_c}
{\Mtrix}
}
~\textbf{\graphGen-asgn}
\and
{
\inferrule
{
{x}^l \in \lvar_c
\and 
\Mtrix = \lMtrix_i * ( \rMtrix_{{\expr}} + \Gamma )
}
{ 
\ad{\Gamma;[ \assign{{x}}{\query({\qexpr})} ]^{l} ; \lvar_c }
{\Mtrix}
}~\textbf{\graphGen-query}}
%
\and 
%
{
\inferrule
{
{\ad{\Gamma + \rMtrix_{{\bexpr}}; {c_1} ; \lvar_c }{ \Mtrix_1}}
\and 
{\ad{\Gamma + \rMtrix_{{\bexpr}}; {c_2}; \lvar_c }{ \Mtrix_2}}
\and
{\Mtrix = (\Mtrix_1 + \Mtrix_2)}
}
{
\ad{\Gamma ; \eif([{\bexpr}]^{l},{ c_1, c_2)}}
{ \Mtrix }
}
~\textbf{\graphGen-if}
}
%
%
%
\and 
%
\inferrule
{
{\ad{\Gamma; {c_1}; \lvar_c }{ \Mtrix_1}  }
\and 
{
\ad{\Gamma;{c_2}; \lvar_c }{ \Mtrix_2}}
}
{
\ad{\Gamma ; ({c_1 ; c_2} ); \lvar_c}
{( \Mtrix_1 {;} \Mtrix_2) }
}
~\textbf{\graphGen-seq}
%
\and 
%
\and 
%
{ 
\inferrule
{
{
\ad{\Gamma + \rMtrix_{{\bexpr}};{c}; \lvar_c  }{ \Mtrix'}
}
}
{
\ad{\Gamma;
\ewhile [ \sbexpr ]^{l} \edo  {c}; \lvar_c }{\Mtrix'}
}~\textbf{\graphGen-while}
}
\end{mathpar}
}
%
Below we define the valid data flow matrix, to have a clear understanding on the data flow generating algorithm:
\begin{defn}[Valid Matrix]
For a labeled variables $\lvar$, $\lvar \vDash (\Mtrix,\flag)$ iff the cardinality of $\lvar$ equals to the one of $\flag$, $|\lvar| = |\flag|$ 
and the matrix $\Mtrix$ is of size $|\flag| \times |\flag|$.
\end{defn}
%
\todo{Improvement if possible: Combining reachability bounds analysis into the static dependency analysis algorithm above, rather than adopting an external tool entirely.}
%
\paragraph{Reachability Bounds}
Given a program $c$ with its labeled variables $\lvar$,
we use the $\rb({x}, {c})$ algorithm, from paper \cite{10.1145/1806596.1806630}, to estimate the reachability bound for each variable ${x} \in \lvar$. 
The input of $\rb$ is a program ${c}$ in SSA language and a variable ${x} $ from ${c}$.
The output of $\rb({x}, {c})$ is an integer representing the reachability bound of ${x}$ in ${c}$.
%

%
The following example programs ${c}2$ and ${c}3$ with while loop illustrate how the algorithm works.
The collected labeled variables, $\lvar_{{c}2}$ and $\lvar_{{c}3}$,
data flow matrix $\Mtrix_{{c}2}$ and  $\Mtrix_{{c}3}$
and variable flags $\flag_{{c}2}$ and $\flag_{{c}3}$
for program ${c}2$ and ${c}3$
are presented in the right hand side.
%
\[
{{c}2 \triangleq
\begin{array}{l}
\left[{ x_1} \leftarrow \query(1)  \right]^1 ; 
\\
\left[{i_1} \leftarrow 0 \right]^2 ; 
\\
\ewhile
~ [{i_1} < 2]^3
	\\
~{[ x_3,x_1 ,x_2 ], [i_3, i_1, i_2] }
~ \edo 
\\
~ \Big( 
\left[{y}_1 \leftarrow \query(2) \right]^4;
\\
\left[{x_2 \leftarrow y_1  + x_3 } \right]^5;
\\
\left[{i_2 \leftarrow 1  + i_3 } \right]^6
\Big) ; 
\\
\left[ {\assign{z_1}{x_3}} + 2  \right]^{7}
\end{array}
,
~~~~
\lvar_{{c}2} = \left [ \begin{matrix}
{x}_1 \\
{x}_3 \\
{y}_1 \\
{x}_2 \\
{z}_1 \\
{i}_1 \\
{i}_2 \\
{i}_3 
\end{matrix} \right ]
% \Mtrix =  \left[ \begin{matrix}
%  & (x_1)  & (y_1) & (x_2) & (x_3) &  (z_1) & i_1 & i_2 & i_3\\
% (x_1) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
% (y_1) & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
% (x_2) & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 \\
% (x_3) & 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
% (z_1) & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
% (i_1) & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
% (i_2) & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
% (i_3) & 1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
% \end{matrix} \right]
,
~~~~~~
\Mtrix_{{c}2} =  \left[ \begin{matrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 \\
1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
1 & 0  & 1& 0 & 0 & 1 & 1 & 1 \\
\end{matrix} \right]
,
~~~~
\flag_{{c}2} = \left [ \begin{matrix}
1 \\
2 \\
1 \\
2 \\
0 \\
0 \\
2 \\
1 
\end{matrix} \right ]
}
\]
%
%
\[
{{{c}3}  \triangleq
\begin{array}{l}
\left[{ x}_1 \leftarrow \query(1)  \right]^1 ;
\\
\left[{i_1} \leftarrow 1 \right]^2 ; 
\\
\ewhile ~ [i < 0]^{3} ,
\\
~{[ x_3,x_1 ,x_2 ], [i_3, i_1, i_2] }
~ \edo
\\
~ \Big( 
\left[{ y_1} \leftarrow \query(2) \right]^3; \\
\left[{x_2 \leftarrow y_1  + x_3 } \right]^5
\Big) ; \\
\left[ {\assign{z_1}{x_3}} + 2  \right]^{6}
\end{array},
~~~~~~
\lvar_{{c}3} = \left [ \begin{matrix}
{x}_1 \\
{i}_1 \\
{x}_3 \\
{i}_3 \\
{z}_1 \\
\end{matrix} \right ]
,~~~~~~
\Mtrix_{{c}3}  =  \left[ \begin{matrix}
0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0 \\
\end{matrix} \right]
,~~~~~~
\flag_{{c}3} = \left [ \begin{matrix}
1 \\
0 \\
2 \\
2 \\
0 \\
\end{matrix} \right ]
}
\]
%
We can now look at the if statement.
\[ 
%
{c}4 \triangleq
\begin{array}{l}
	\left[ {x}_1 \leftarrow \query(1) \right]^1; 
	\\
	\left[{y}_1 \leftarrow \query(2) \right]^2 ; 
	\\
\eif \;( { x_1 + y_1 == 5} )^3,  \\
{[ x_4,x_2,x_3 ],[] ,[y_3,y_1,y_2 ]} 
\\
\mathsf{then} ~ \left[ 
{x}_2 \leftarrow \query(3) \right]^4 
\\
\mathsf{else} ~ \left[ 
{x}_3 \leftarrow \query(4) \right]^5 ; 
\\
{y}_2 \leftarrow 2 ) \\
\left[ { z_1 \leftarrow x_4 +y_3 }\right]^6
\end{array},
% \]
% \[
~~~~~~
\lvar_{{c}4} =  \left[ \begin{matrix}
{x}_1 \\
{y}_1 \\
{x}_2 \\
{x}_3 \\
{y}_2 \\
{x}_4 \\
{y}_3 \\
{z}_1 \\
\end{matrix} \right], 
~~~~~ 
\Mtrix_{{c}4} =  \left[ \begin{matrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
\end{matrix} \right], 
~~~~~ 
\flag_{{c}4} = \left [ \begin{matrix}
1 \\
1 \\
1 \\
1 \\
0 \\
0 \\
0 \\
0 \\
\end{matrix} \right ]
\]
%
%
%
%
\subsection{Adaptivity Based on Program Analysis in \THESYSTEM}
%
\begin{defn}
[Program-Based Dependency Graph].
\label{def:prog-based_graph}
\\
Given a program ${c}$ with its labeled variables $\lvar$ of length $N$, s.t.,
$\Gamma \vdash_{\Mtrix_c, \flag_c} {c}$, 
its program-based graph 
$G({c}) = (\vertxs, \edges, \weights, \flag)$ is. defined as:
\\
\[
\begin{array}{rlcl}
\text{Vertices} &
\vertxs & := & \left\{ 
x^l \in \mathcal{VAR} \times \mathbb{N}
~ \middle\vert ~
x^l \in \lvar({{c}})
\right\}
\\
\text{Directed Edges} &
\edges & := & 
\left\{ 
  ({x}_1^{i}, {x}_2^{j}) \in \mathcal{VAR} \times \mathbb{N} \times (\mathcal{VAR} \times \mathbb{N})
  ~ \middle\vert ~
 	{x}_1^{i}, {x}_2^{j} \in \lvar_{{c}}
	\land \Mtrix_c(i, j) \geq 1
\right\}
\\
\text{Weights} &
\weights & := &
\bigcup
\begin{array}{l}
	\big\{ (x^l, n) \in \mathcal{VAR} \times \mathbb{N} \times (\mathbb{N} \cup \expr)
	\mid
	x^l \in \lvar_{{c}} \land \flag(x^l) > 0 \land w = \rb(x^l, c)
	\big\} 
	\\
	\big\{(x^l, 1)  \in \mathcal{VAR} \times \mathbb{N} \times \{1\} 
	\mid
	x^l \in \lvar_{{c}} \land \flag(x^l) = 0
	\big\}
\end{array} 
\\
\text{Query Flags} &
\qflag & := & 
\big\{({x}, n)  \in \vertxs \times \{0, 1\} 
\mid 
\left\{
\begin{array} {ll}
n = 1 & \flag_c(i) = 2
\\  
n = 0 & o.w.
\end{array}
\right\};
{x} = \lvar(i); i = 1, \ldots, N
\big\}
\end{array}
\]
\end{defn} 
%
Given a program ${c}$, we generate its program-based graph 
$\progG({c}) = (\vertxs, \edges, \weights, \qflag)$.
%
Then the adaptivity bound based on program analysis for ${c}$ is the number of query vertices on a finite walk in $\progG({c})$. This finite walk satisfies:
\begin{itemize}
\item the number of query vertices on this walk is maximum
\item the visiting times of each vertex $v$ on this walk is bound by its reachability bound $\weights(v)$.
\end{itemize}
It is formally defined in \ref{def:prog_adapt}.
%
%
\begin{defn}
[{Program-Based Adaptivity}].
\label{def:prog_adapt}
\\
{
Given a program ${c}$ and its program-based graph 
$\progG({c}) = (\vertxs, \edges, \weights, \qflag)$,
%
the program-based adaptivity for $c$ is defined as%
\[
\progA({c}) 
:= \max
\left\{ \qlen(k)\ \mid \  k\in \walks(\progG({c}))\right \}.
\]
}
\end{defn}  

% By specifying the departure and destination vertices $s$ and $t$, the $\pathssearch(\progG, s, t)$ algorithm will 
% give the number of query vertices on a finite walk from $s$ to $t$, which contains the maximum number of query vertices.
% The pseudo-code of $\pathssearch(\progG, s, t)$ algorithm is defined in the Algorithm \ref{alg:adpt_alg}.
% %
% \begin{algorithm}
% \caption{
% {Walk Search Algorithm ($\pathssearch$)}
% \label{alg:adpt_alg}
% }
% \begin{algorithmic}
% \REQUIRE Weighted Directed Graph $G = (\vertxs, \edges, \weights, \flag)$ with a start vertex $s$ and destination vertex $t$ .
% \STATE  {\bf {bfs $(G, s, t)$}:}  
% \STATE \qquad {\bf init} 
% current node: $c = s$, 
% queue: $q = [c]$, 
% vector recoding if the vertex is visited: 
% visited$ = [0]*|\vertxs|$,
% result: $r$
% \STATE \qquad {\bf while} $q$ isn't empty:
% \STATE \qquad \qquad take the vertex from beginning $c= q.pop()$
% \STATE \qquad \qquad mark $c$ as visited, visited $[c] = 1$
% \STATE \qquad \qquad currMinFlow = min($\weights$(c), currMinFlow).
% \STATE \qquad \qquad put all unvisited vertex $v$ having directed edge from c into $q$. 
% \STATE \qquad \qquad if $v$ is visited, then there is a circle in the graph, we update the result $r = r + $currMinFlow
% \RETURN $r$
% \end{algorithmic}
% \end{algorithm}
%
%
% \subsection{\todo{Soundness of the \THESYSTEM}}

% {
% 	\begin{thm}[Soundness of the \THESYSTEM].
% 	Given a program ${c}$, we have:
% 	%
% 	\[
% 	\progA({c}) \geq A({c}).
% 	\]
% 	\end{thm}
% }
% {
% \begin{proof}
% Given a program ${c}$, 
% we construct its program-based graph $\progG({c}) = (\vertxs, \edges, \weights, \qflag)$
% by Definition~\ref{def:prog-based_graph}
% According to the Definition \ref{def:prog_adapt}, we have:
% %
% \[
% 	\progA({c}) 
% 	:= \max\left\{ \qlen(k)\ \mid \  k\in \walks(\progG({c}))\right \}.
% \]
% %
% According to the Definition \ref{def:trace-based_adapt}, we have the trace-based adaptivity as follows:
% $$
% A({c}) = \max \big 
% \{ \len(p) \mid {m} \in \mathcal{SM},D \in \dbdom ,p \in \paths(\traceG({c}, \text{D}, {m}) \big \} 
% $$
% %
% Then, we need to show:
% \[
% \max \big 
% \{ \len(p) \mid {m} \in \mathcal{SM},D \in \dbdom ,p \in \paths(\traceG({c}, \text{D}, {m}) \big \} 
% \leq
% \max\left\{ \qlen(k) \ \mid \  k\in \walks(\progG({c}))\right \}
% \]
% %
% It is sufficient to show that:
% \[
% 	\forall p, {m}, D, ~ s.t., ~ p \in \paths(\traceG({c}, \text{D}, {m}),
% 	\exists k \in \walks(\progG({c})) \land 
% 	\len(p) \leq \qlen(k)
% \]
% %
% Taking an arbitrary starting memory $m$ and an arbitrary underlying database $D$,
% we construct a trace-based graph $\traceG({c}, \text{D}, {m}) = (\vertxs, \edges)$ by the definition \ref{def:trace-based_graph}.
% %
% \\
% %
% Let $\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$ be the intermediate graph by Definition~\ref{def:midgraph}.
% \\
% By Lemma~\ref{lem:bie_trace_to_mid}, we know:
% \[
% 	\forall p, {m}, D, ~ s.t., ~ p \in \paths(\traceG({c}, \text{D}, {m}),
% 	\exists p' \in \paths(\midG({c},{m},\text{D})) \land 
% 	\len(p) = \len_q(p')
% \]
% %
% Then it is sufficient to show that:
% %
% \[
% 	\forall p, {m}, D, ~ s.t., ~ p \in \paths(\midG({c}, \text{D}, {m}),
% 	\exists k \in \walks(\progG({c})) \land 
% 	\qlen(p) \leq \qlen(k)
% \]
% %
% We prove a stronger statement instead:
% \[
% 	\forall p, {m}, D, ~ s.t., ~ p \in \paths(\midG({c}, \text{D}, {m}),
% 	\exists k \in \walks(\progG({c})) \land 
% 	\qlen(p) = \qlen(k)	
% \]
% %
% %
% By Lemma~\ref{lem:sujv_mid_to_prog}, let $g$ be the surjective function $g: \progV \to \midV$ s.t.:
% %
% $$
% \forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) 
% \land |\kw{image}(f(\av))| \leq W(f(\av)).
% $$
% %
% %
% % \item(1) $\len(p_{\av_1 \to \av_2}) = \len(k_{f(\av_1) \to f(\av_2)})$
% % %
% % \item(2) $\forall \av \in p_{\av_1 \to \av_2}. ~ f(\av) \in k_{f(\av_1) \to f(\av_2)}$
% % %
% % \item(3) $\forall \av \in p_{\av_1 \to \av_2}. ~ 
% % \kw{image}(f(\av)) \cap {p_{\av_1 \to \av_2}}| = \# \{f(\av) \mid f(\av) \in k_{f(\av_1) \to f(\av_2)}\}$
% %
% Let ${m}$ and $D$ be an arbitrary memory and database $D$,
% taking an arbitrary path $p_{\av_1 \to \av_n} \in \paths(\midG({c}, \text{D}, {m})$ with:
% %
% \item Edge sequence: $(e, \ldots, e_{n-1})$
% %
% \item Vertices sequence: $(\av_1, \ldots, \av_n)$.
% \\
% By Lemma~\ref{lem:sujpathwalk_mid_to_prog}, let $h: \paths(\midG({c}, \text{D}, {m})) \to \walks(\progG({c}))$ be the surjective function satisfies:
% %
% \[
% 	\forall p_{\av_1 \to \av_n} \in \paths(\midG({c}, \text{D}, {m}))
% 	\text{ with }
% 	\left\{
% 	\begin{array}{ll}
% 	\mbox{edge sequence:} & (e, \ldots, e_{n-1})
% 	\\ 
% 	\mbox{vertices sequence:} & (\av_1, \ldots, \av_n)
% 	\end{array}
% 	\right.
% \]
% %
% \[
% 	\exists k_{f(\av_1) \to f(\av_n)} \in \walks(\progG({c}))
% 	\text{ with }
% 	\left\{
% 	\begin{array}{ll}
% 	\mbox{edge sequence:} & (g(e), \ldots, g(e_{n-1}) 
% 	\\ 
% 	\mbox{vertices sequence:} & (f(\av_1), \ldots, f(\av_{n}))
% 	\end{array}
% 	\right.
% \]
% %
% We have the walk:
% $k_{f(\av_1) \to f(\av_n)} \in \walks(\progG({c}))$ with:
% %
% \item Edges sequence: $(g(e), \ldots, g(e_{n-1}) $
% %
% \item Vertices sequence: $(f(\av_1), \ldots, f(\av_{n}))$.
% \\
% It is sufficient to show 
% %
% \[
% 	\qlen(p_{\av_1 \to \av_n}) = \qlen(k_{f(\av_1) \to f(\av_n)})
% \]
% %
% Unfold the definition of $\qlen$, it is suffice to show:
% \[
% \len \big( \av \mid \av \in (\av_1, \ldots, \av_n) \land \midF(\av) = 2 \big) 
% = \len \big(f(\av) \mid f(\av) \in (f(\av_1), \ldots, f(\av_{n})) \land \progF(f(\av)\big) = 2)	
% ~ (a)
% \]
% %
% By Lemma~\ref{lem:sujv_mid_to_prog}, we know:
% %
% \[
% 	\forall \av \in \midV. ~ \midF(\av) = \progF(f(\av)) ~(b)
% \]
% By rewriting $(b)$ in $(a)$, we have this case proved.
% %
% \\
% \todo{
% \begin{defn}[Intermediate Graph $\midG$].
% 	\label{def:midgraph}
% 	\\
% 	$\mathcal{AV}$ : Annotated Variables based on program execution
% 	\\
% 	Given a program ${c}$ with its labeled variables $\lvar$ of length $N$,
% 	a database $D$, a starting memory ${m}$,
% 	s.t., $\Gamma \vdash_{\Mtrix_c, \flag_c} {c}$,
% 	the intermediate graph 
% 	$\midG({c},{m},\text{D}) = (\vertxs, \edges, \flag)$ is defined as:%
% \[
% \begin{array}{rlcl}
% 	\text{Vertices} &
% 	\vertxs & := & \left\{ 
% 	\av \in \mathcal{AV} \middle\vert
% 	\exists {m'},  w', \qtrace, \vtrace.  ~ s.t., ~  
% 	\config{{m} ,{c}, [], [], []}  \to^{*}  \config{{m'} , \eskip, \qtrace, \vtrace, w' }
% 	\land \av \in \vtrace
% 	\right\}
% 	\\
% 	\text{Directed Edges} &
% 	\edges & := & 
% 	\left\{ 
% 	(\av, \av') \in \mathcal{AV} \times \mathcal{AV} 
% 	~ \middle\vert ~
% 	\flowsto(\av, \av', {c},{m},D) 
% 	\right\}
% 	\\
% 	\text{Flags} &
% 	\flag & := & 
% 	\big\{ (\av, n)  \in \vertxs \times \{0, 1, 2\} 
% 	\mid 
% 	(\pi_1(\av) = \lvar(i) \land n = \flag_c(i)); ~
% 	i = 1, \ldots, N
% 	\big\}
% \end{array}
% \]
% \end{defn}
% }
% %
% \\
% \todo{
% 	\begin{lem}[$\vardep$ is Transitive].
% 	\label{lem:vardep_trans}
% 	\\
% 	Given a program ${c}$, with a starting memory ${m}$ and a hidden database $D$, s.t., 
% 	$\config{{m}, {c}, [], [], []} \rightarrow^{*} \config{{m}', \eskip, \qtrace, \vtrace, w} $.
% 	Then, $\forall \av_1, \av_2, \av_3 \in \vtrace$:
% \[
% 	\Big(\vardep(\av_1, \av_2, {c}, {m}, D) \land 
% 	\vardep(\av_2, \av_3, {c}, {m}, D) \Big)
% 	\implies
% 	\vardep(\av_1, \av_3, {c}, {m}, D)
% \]
% 	\end{lem}
% 	\begin{subproof}[of Lemma~\ref{lem:vardep_trans}]
% 	Proof by unfolding and rewriting the Definition~\ref{def:var_dep}.
% 	\end{subproof}
% }
% \\
% %
% \todo{
% 	\begin{lem}[$\flowsto$ is Transitive ??].
% 	\label{lem:flowsto_trans}
% 	\\
% 	Given a program ${c}$ with its labeled variables $\lvar$ of length $N$. 
% 	Then $\forall x_1, x_2, x_3 \in \lvar$
% \[
% 	\Big(\flowsto(x_1, x_2) \land \flowsto(x_2, x_3) \Big)
% 	\implies
% 	\flowsto(x_1, x_3)
% \]
% 	\end{lem}
% 	\begin{subproof}[of Lemma~\ref{lem:flowsto_trans}]
% 	Proof by unfolding the Definition~\ref{def:flowsto}.
% 	\end{subproof}
% }
% \\
% %
% \todo{
% 	\begin{lem}[$\qdep$ Implies $\vardep$].
% 	\label{lem:querydep_vardep}
% 	\\
% 	Given a program ${c}$, with a starting memory ${m}$ and a hidden database $D$, s.t., 
% 	$\config{{m}, {c}, [], [], []} \rightarrow^{*} \config{{m}', \eskip, \qtrace, \vtrace, w} $.
% 	Then, $\forall \av_1, \av_2 \in \qtrace$
% \[
% 	\qdep(\av_1, \av_2, {c}, {m}, D) \implies 
% 	\vardep(\pi_2(\av_1), \pi_2(\av_2), {c}, {m}, D)
% \]
% 	\end{lem}
% 	\begin{subproof}[of Lemma~\ref{lem:querydep_vardep}]
% 	Proof by unfolding the Definition~\ref{def:var_dep} and Definition~\ref{def:query_dep}.
% 	\end{subproof}
% }
% \\
% %
% \todo{
% 	\begin{lem}[$\vardep$ Implies \flowsto].
% 	\label{lem:vardep_flows}
% 	\\
% 	Given a program ${c}$, with a starting memory ${m}$ and a hidden database $D$, s.t., 
% 	$\config{{m}, {c}, [], [], []} \rightarrow^{*} \config{{m}', \eskip, \qtrace, \vtrace, w} $.
% 	Then, $\forall \av_1, \av_2 \in \vtrace$
% \[
% 	\vardep(\av_1, \av_2, {c}, {m}, D) \implies 
% 	\flowsto(\pi_1(\av_1), \pi_1(\av_2))
% \]
% 	\end{lem}
% 	\begin{subproof}[of Lemma~\ref{lem:querydep_vardep}]
% 	Proof by showing contradiction based on the Definition~\ref{def:var_dep} and Definition~\ref{def:flowsto}.
% 	Let $\av_1, \av_2 \in \vtrace$ be 2 arbitrary annotated variables in the variable trace $\vtrace$,
% 	s.t., $\vardep(\av_1, \av_2, {c}, {m}, D)$.
% 	\\
% 	Unfolding the $\vardep$ definition, we have:	
% 	\end{subproof}
% }
% \\
% %
% \todo{
% 	\begin{lem}[Injective Mapping of vertices from $\traceG$ to $\midG$].
% 	\label{lem:injv_trace_to_mid}
% 	\\
% 	$\traceG({c}) = \{\traceV, \traceE\}$
% 	\\
% 	$\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$
% \[
% 	\exists ~ \kw{injective} ~ f: \mathcal{AQ} \to \mathcal{AV}. 
% 	~ \forall \av \in \traceV. ~ 
% 	f(\av) \in \midV \land \midF(f(\av)) = 2
% \]
% 	\end{lem}
% \begin{subproof}
% Proving by Definition~\ref{def:midgraph} and Definition~\ref{def:prog_adapt}.
% \end{subproof}
% }
% \\
% \todo{
% 	\begin{lem}[One-on-One Mapping from $\edges$ of $\traceG$ to $\paths(\midG)$].
% 	\label{lem:bie_trace_to_mid}
% 	\\
% 	$\traceG({c}) = \{\traceV, \traceE\}$
% 	\\
% 	$\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$
% 	\\
% 	An injective function $ f: \traceV \to \midV$ s.t.,
% 	$\forall \av \in \traceV. ~ \midF(f(\av)) = 2$ 
% \[
% 	\forall e = (\av_1, \av_2) \in \traceE. ~ 
% 	\exists p_{f(\av_1) \to f(\av_2)} \in \paths(\midG({c}, \text{D}, {m}))
% \]
% 	\end{lem}
% \begin{subproof}
% Proving by Lemma~\ref{lem:injv_trace_to_mid} and Definition~\ref{def:midgraph} and acyclic property of $\traceG$ and $\midG$.
% \end{subproof}
% }
% \\
% \todo{
% 	\begin{lem}[Surjective Mapping of Vertices from $\midG$ to $\progG$].
% 	\label{lem:sujv_mid_to_prog}
% 	\\
% 	$\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$
% 	\\
% 	$\progG({c}) = \{\progV, \progE, \progF, \progW\}$
% 	\\
% 	$\exists ~ \kw{surjective} ~ f: \mathcal{AV} \to \mathcal{SVAR}.$
% 	%
% \[
% 	\forall \av \in \midV. ~ 
% 	f(\av) \in \progV \land \progF(f(\av)) = \midF(\av) \land
% 	|\kw{image}(f(\av))| \leq W(f(\av))
% \]
% \end{lem}
% \begin{subproof}
% Proving by Definition~\ref{def:midgraph}.
% \end{subproof}
% }
% \\
% \todo{
% 	\begin{lem}[Surjective Mapping from $\edges$ of $\midG)$ to $\edges$ of $\progG$].
% 	\label{lem:suje_mid_to_prog}
% 	\\
% 	$\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$
% 	\\
% 	$\progG({c}) = \{\progV, \progE, \progF, \progW\}$
% 	\\
% 	A surjective function $f: \progV \to \midV$ s.t.,
% 	$\forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) \land |\kw{image}(f(\av))| \leq W(f(\av))$
% 	%
% \[
% 	\exists ~ \kw{surjective} ~ g: \midE \to \progE. ~
% 	\forall e_{mid} = (\av_1, \av_2) \in \midE. 
% 	\exists e_{prog} = ({f(\av_1), f(\av_2)}) \in \progE
% \]
% \end{lem}
% \begin{subproof}
% Proving by Lemma~\ref{lem:sujv_mid_to_prog}.
% \end{subproof}
% }
% \\
% \todo{
% 	\begin{lem}[Surjective Mapping from $\paths(\midG)$ to $\walks(\progG)$].
% 	\label{lem:sujpathwalk_mid_to_prog}
% 	\\
% 	$\midG({c},{m},\text{D}) = \{\midV, \midE, \midF\}$
% 	\\
% 	$\progG({c}) = \{\progV, \progE, \progF, \progW\}$
% 	\\
% 	A surjective function $f: \progV \to \midV$ s.t.,
% 	$\forall \av \in \midV. ~ \progF(f(\av)) = \midF(\av) \land |\kw{image}(f(\av))| \leq W(f(\av))$
% 	\\
% 	A surjective function $g: \midE \to \progE$ s.t.,
% 	$\forall e_{mid} = (\av_1, \av_2) \in \midE. 
% 	\exists e_{prog} = ({f(\av_1) \to f(\av_2)}) \in \progE$
% 	\\
% 	$\exists ~ \kw{surjective} ~ h: \paths(\midG({c},{m},\text{D})) \to \walks(\progG({c}))$ s.t.:
% 	%
% \[
% 	\forall p_{\av_1 \to \av_2} \in \paths(\midG({c},{m},\text{D}))
% 	\text{ with }
% 	\left\{
% 	\begin{array}{ll}
% 	\mbox{edge sequence:} & (e, \ldots, e_{n-1})
% 	\\ 
% 	\mbox{vertices sequence:} & (\av_1, \ldots, \av_n)
% 	\end{array}
% 	\right.
% \]
% \[
% 	\exists k_{f(\av_1) \to f(\av_2)} \in \walks(\progG({c}))
% 	\text{ with }
% 	\left\{
% 	\begin{array}{ll}
% 	\mbox{edge sequence:} & (g(e), \ldots, g(e_{n-1}) 
% 	\\ 
% 	\mbox{vertices sequence:} & (f(\av_1), \ldots, f(\av_{n}))
% 	\end{array}
% 	\right.
% \]
% % \item $(e, \ldots, e_{n-1})$, $(\av_1, \ldots, \av_n)$ are the edges sequence and vertices sequence of $p_{\av_1 \to \av_2}$.
% % then, 
% %  $\len(p_{\av_1 \to \av_2}) = \len(k_{f(\av_1) \to f(\av_2)})$
% % %
% % \item $\forall \av \in p_{\av_1 \to \av_2}. ~ f(\av) \in k_{f(\av_1) \to f(\av_2)}$
% % %
% % \item $\forall \av \in p_{\av_1 \to \av_2}. ~ 
% % \kw{image}(f(\av)) \cap {p_{\av_1 \to \av_2}}| = \# \{f(\av) \mid f(\av) \in k_{f(\av_1) \to f(\av_2)}\}
% % $
% \end{lem}
% %
% \begin{subproof}
% Proving by induction on the length of $l = p_{\av_1 \to \av_2} \in \paths(\midG({c},{m},\text{D}))$, and Lemma~\ref{lem:suje_mid_to_prog} and Lemma~\ref{lem:sujv_mid_to_prog}.
% \caseL{ $l = 1$: }
% \caseL{ $l = l' + 1$, $l' \geq 1$: }
% \end{subproof}
% }
% \end{proof}
% %

% %
% }